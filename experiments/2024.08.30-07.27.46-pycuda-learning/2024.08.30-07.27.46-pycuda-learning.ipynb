{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[pycuda learning](https://github.com/apowers313/roc/blob/master/experiments/2024.08.30-07.27.46-pycuda-learning/2024.08.30-07.27.46-pycuda-learning.ipynb)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 31 15:35:38 PDT 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":/usr/local/cuda/bin\"\n",
    "print(\"PATH:\", os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  2 12:12:12 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   50C    P8              4W /  220W |       2MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Mon_Oct_24_19:12:58_PDT_2022\n",
      "Cuda compilation tools, release 12.0, V12.0.76\n",
      "Build cuda_12.0.r12.0/compiler.31968024_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 CUDA Capable device(s)\n",
      "Device NVIDIA GeForce RTX 4070 SUPER:\n",
      "\tCompute Capability: 8.9\n",
      "\tTotal Memory: 12002\n",
      "\t(56) Multiprocessors, (64) CUDA Cores / Multiprocessor: 3584 CUDA Cores\n",
      "\tASYNC_ENGINE_COUNT: 2\n",
      "\tCAN_MAP_HOST_MEMORY: 1\n",
      "\tCAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 1\n",
      "\tCLOCK_RATE: 2475000\n",
      "\tCOMPUTE_CAPABILITY_MAJOR: 8\n",
      "\tCOMPUTE_CAPABILITY_MINOR: 9\n",
      "\tCOMPUTE_MODE: DEFAULT\n",
      "\tCOMPUTE_PREEMPTION_SUPPORTED: 1\n",
      "\tCONCURRENT_KERNELS: 1\n",
      "\tCONCURRENT_MANAGED_ACCESS: 1\n",
      "\tDIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0\n",
      "\tECC_ENABLED: 0\n",
      "\tGENERIC_COMPRESSION_SUPPORTED: 1\n",
      "\tGLOBAL_L1_CACHE_SUPPORTED: 1\n",
      "\tGLOBAL_MEMORY_BUS_WIDTH: 192\n",
      "\tGPU_OVERLAP: 1\n",
      "\tHANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1\n",
      "\tHANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0\n",
      "\tHANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0\n",
      "\tHOST_NATIVE_ATOMIC_SUPPORTED: 0\n",
      "\tINTEGRATED: 0\n",
      "\tKERNEL_EXEC_TIMEOUT: 0\n",
      "\tL2_CACHE_SIZE: 50331648\n",
      "\tLOCAL_L1_CACHE_SUPPORTED: 1\n",
      "\tMANAGED_MEMORY: 1\n",
      "\tMAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE1D_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE2D_WIDTH: 131072\n",
      "\tMAXIMUM_SURFACE3D_DEPTH: 16384\n",
      "\tMAXIMUM_SURFACE3D_HEIGHT: 16384\n",
      "\tMAXIMUM_SURFACE3D_WIDTH: 16384\n",
      "\tMAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\tMAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
      "\tMAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE3D_DEPTH: 16384\n",
      "\tMAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
      "\tMAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
      "\tMAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
      "\tMAXIMUM_TEXTURE3D_WIDTH: 16384\n",
      "\tMAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
      "\tMAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\tMAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
      "\tMAX_BLOCKS_PER_MULTIPROCESSOR: 24\n",
      "\tMAX_BLOCK_DIM_X: 1024\n",
      "\tMAX_BLOCK_DIM_Y: 1024\n",
      "\tMAX_BLOCK_DIM_Z: 64\n",
      "\tMAX_GRID_DIM_X: 2147483647\n",
      "\tMAX_GRID_DIM_Y: 65535\n",
      "\tMAX_GRID_DIM_Z: 65535\n",
      "\tMAX_PERSISTING_L2_CACHE_SIZE: 34603008\n",
      "\tMAX_PITCH: 2147483647\n",
      "\tMAX_REGISTERS_PER_BLOCK: 65536\n",
      "\tMAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "\tMAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "\tMAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 101376\n",
      "\tMAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 102400\n",
      "\tMAX_THREADS_PER_BLOCK: 1024\n",
      "\tMAX_THREADS_PER_MULTIPROCESSOR: 1536\n",
      "\tMEMORY_CLOCK_RATE: 10501000\n",
      "\tMEMORY_POOLS_SUPPORTED: 1\n",
      "\tMULTI_GPU_BOARD: 0\n",
      "\tMULTI_GPU_BOARD_GROUP_ID: 0\n",
      "\tPAGEABLE_MEMORY_ACCESS: 0\n",
      "\tPAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0\n",
      "\tPCI_BUS_ID: 1\n",
      "\tPCI_DEVICE_ID: 0\n",
      "\tPCI_DOMAIN_ID: 0\n",
      "\tREAD_ONLY_HOST_REGISTER_SUPPORTED: 1\n",
      "\tRESERVED_SHARED_MEMORY_PER_BLOCK: 1024\n",
      "\tSINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 64\n",
      "\tSTREAM_PRIORITIES_SUPPORTED: 1\n",
      "\tSURFACE_ALIGNMENT: 512\n",
      "\tTCC_DRIVER: 0\n",
      "\tTEXTURE_ALIGNMENT: 512\n",
      "\tTEXTURE_PITCH_ALIGNMENT: 32\n",
      "\tTOTAL_CONSTANT_MEMORY: 65536\n",
      "\tUNIFIED_ADDRESSING: 1\n",
      "\tWARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/Chapter03/deviceQuery.py\n",
    "\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "\n",
    "print(f\"Detected {drv.Device.count()} CUDA Capable device(s)\")\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "    gpu_device = drv.Device(i)\n",
    "    print(f\"Device {gpu_device.name()}:\")\n",
    "    compute_capability = float(\"%d.%d\" % gpu_device.compute_capability())\n",
    "    print(f\"\\tCompute Capability: {compute_capability}\")\n",
    "    print(f\"\\tTotal Memory: {gpu_device.total_memory()//(1024**2)}\")\n",
    "\n",
    "    raw_device_attributes = gpu_device.get_attributes()\n",
    "    device_attributes = {str(k): raw_device_attributes[k] for k in raw_device_attributes.keys()}\n",
    "\n",
    "    num_mp = device_attributes[\"MULTIPROCESSOR_COUNT\"]\n",
    "\n",
    "    major_compute_capability = gpu_device.compute_capability()[0]\n",
    "    cuda_cores_per_mp = {\n",
    "        # Maxwell\n",
    "        5.0: 128,\n",
    "        5.1: 128,\n",
    "        5.2: 128,\n",
    "        # Pascal\n",
    "        6.0: 64,\n",
    "        6.1: 128,\n",
    "        6.2: 128,\n",
    "        # Volta and Turing\n",
    "        7.0: 64,\n",
    "        7.5: 64,\n",
    "        # Ampere\n",
    "        8.0: 64,\n",
    "        8.6: 128,\n",
    "        8.9: 128,  # Ada Lovelace\n",
    "        # Hopper\n",
    "        9.0: 128,\n",
    "    }[major_compute_capability]\n",
    "\n",
    "    # RTX 4070 SUPER should have 7168 CUDA Cores, this reports 3584 which isn't right\n",
    "    # https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4070-family/\n",
    "    print(\n",
    "        f\"\\t({num_mp}) Multiprocessors, ({cuda_cores_per_mp}) CUDA Cores / Multiprocessor: {num_mp*cuda_cores_per_mp} CUDA Cores\"\n",
    "    )\n",
    "\n",
    "    device_attributes.pop(\"MULTIPROCESSOR_COUNT\")\n",
    "\n",
    "    for k in device_attributes.keys():\n",
    "        print(f\"\\t{k}: {device_attributes[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "\n",
    "host_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "device_data_x2 = 2 * device_data\n",
    "host_data_x2 = device_data_x2.get()\n",
    "print(host_data_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from thread 0, in block 1!\n",
      "Hello world from thread 1, in block 1!\n",
      "Hello world from thread 2, in block 1!\n",
      "Hello world from thread 3, in block 1!\n",
      "Hello world from thread 4, in block 1!\n",
      "Hello world from thread 0, in block 0!\n",
      "Hello world from thread 1, in block 0!\n",
      "Hello world from thread 2, in block 0!\n",
      "Hello world from thread 3, in block 0!\n",
      "Hello world from thread 4, in block 0!\n",
      "-------------------------------------\n",
      "This kernel was launched over a grid consisting of 2 blocks,\n",
      "where each block has 5 threads.\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ker = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void hello_world_ker()\n",
    "{\n",
    "\tprintf(\"Hello world from thread %d, in block %d!\\\\n\", threadIdx.x, blockIdx.x);\n",
    "\t\n",
    "\t__syncthreads();\n",
    "\t\n",
    "\tif(threadIdx.x == 0 && blockIdx.x == 0)\n",
    "\t{\n",
    "\t\tprintf(\"-------------------------------------\\\\n\");\n",
    "\t\tprintf(\"This kernel was launched over a grid consisting of %d blocks,\\\\n\", gridDim.x);\n",
    "\t\tprintf(\"where each block has %d threads.\\\\n\", blockDim.x);\n",
    "\t}\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "hello_ker = ker.get_function(\"hello_world_ker\")\n",
    "hello_ker(block=(5, 1, 1), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does our kernel work correctly? : True\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/Chapter04/simple_scalar_multiply_kernel.py\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ker = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void scalar_multiply_kernel(float *outvec, float scalar, float *vec)\n",
    "{\n",
    "     int i = threadIdx.x;\n",
    "     outvec[i] = scalar*vec[i];\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "scalar_multiply_gpu = ker.get_function(\"scalar_multiply_kernel\")\n",
    "\n",
    "testvec = np.random.randn(512).astype(np.float32)\n",
    "testvec_gpu = gpuarray.to_gpu(testvec)\n",
    "outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
    "\n",
    "scalar_multiply_gpu(outvec_gpu, np.float32(2), testvec_gpu, block=(512, 1, 1), grid=(1, 1, 1))\n",
    "\n",
    "print(f\"Does our kernel work correctly? : {np.allclose(outvec_gpu.get() , 2*testvec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0, 0): Block (1, 0, 0), Thread (0, 0, 0)\n",
      "(5, 0, 0): Block (1, 0, 0), Thread (1, 0, 0)\n",
      "(6, 0, 0): Block (1, 0, 0), Thread (2, 0, 0)\n",
      "(7, 0, 0): Block (1, 0, 0), Thread (3, 0, 0)\n",
      "(4, 1, 0): Block (1, 0, 0), Thread (0, 1, 0)\n",
      "(5, 1, 0): Block (1, 0, 0), Thread (1, 1, 0)\n",
      "(6, 1, 0): Block (1, 0, 0), Thread (2, 1, 0)\n",
      "(7, 1, 0): Block (1, 0, 0), Thread (3, 1, 0)\n",
      "(4, 0, 1): Block (1, 0, 0), Thread (0, 0, 1)\n",
      "(5, 0, 1): Block (1, 0, 0), Thread (1, 0, 1)\n",
      "(6, 0, 1): Block (1, 0, 0), Thread (2, 0, 1)\n",
      "(7, 0, 1): Block (1, 0, 0), Thread (3, 0, 1)\n",
      "(4, 1, 1): Block (1, 0, 0), Thread (0, 1, 1)\n",
      "(5, 1, 1): Block (1, 0, 0), Thread (1, 1, 1)\n",
      "(6, 1, 1): Block (1, 0, 0), Thread (2, 1, 1)\n",
      "(7, 1, 1): Block (1, 0, 0), Thread (3, 1, 1)\n",
      "(0, 0, 0): Block (0, 0, 0), Thread (0, 0, 0)\n",
      "(1, 0, 0): Block (0, 0, 0), Thread (1, 0, 0)\n",
      "(2, 0, 0): Block (0, 0, 0), Thread (2, 0, 0)\n",
      "(3, 0, 0): Block (0, 0, 0), Thread (3, 0, 0)\n",
      "(0, 1, 0): Block (0, 0, 0), Thread (0, 1, 0)\n",
      "(1, 1, 0): Block (0, 0, 0), Thread (1, 1, 0)\n",
      "(2, 1, 0): Block (0, 0, 0), Thread (2, 1, 0)\n",
      "(3, 1, 0): Block (0, 0, 0), Thread (3, 1, 0)\n",
      "(0, 0, 1): Block (0, 0, 0), Thread (0, 0, 1)\n",
      "(1, 0, 1): Block (0, 0, 0), Thread (1, 0, 1)\n",
      "(2, 0, 1): Block (0, 0, 0), Thread (2, 0, 1)\n",
      "(3, 0, 1): Block (0, 0, 0), Thread (3, 0, 1)\n",
      "(0, 1, 1): Block (0, 0, 0), Thread (0, 1, 1)\n",
      "(1, 1, 1): Block (0, 0, 0), Thread (1, 1, 1)\n",
      "(2, 1, 1): Block (0, 0, 0), Thread (2, 1, 1)\n",
      "(3, 1, 1): Block (0, 0, 0), Thread (3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void test_kernel()\n",
    "{\n",
    "    #define MY_X (threadIdx.x + blockIdx.x * blockDim.x)\n",
    "    #define MY_Y (threadIdx.y + blockIdx.y * blockDim.y)\n",
    "    #define MY_Z (threadIdx.z + blockIdx.z * blockDim.z)\n",
    "\n",
    "    //printf(\"gridDim %d %d %d, warp %d\\\\n\", gridDim.x, gridDim.y, gridDim.z, warpSize);\n",
    "    //printf(\"Hello world from thread %d, in block %d!\\\\n\", threadIdx.x, blockIdx.x);\n",
    "    printf(\"(%d, %d, %d): Block (%d, %d, %d), Thread (%d, %d, %d)\\\\n\", MY_X, MY_Y, MY_Z, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y, threadIdx.z);\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "test_kernel = mod.get_function(\"test_kernel\")\n",
    "test_kernel(block=(4, 2, 2), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter\n",
    "At this point I could make a PyCUDA cell magic :)\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/config/custommagics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_magic, register_cell_magic, register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def cmagic(line, cell):\n",
    "    \"my cell magic\"\n",
    "    return line, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('foo', '\\nthis is a test\\n\\nfoo bar\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cmagic foo\n",
    "\n",
    "this is a test\n",
    "\n",
    "foo bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def cuda(line, cell):\n",
    "    \"Runs NVIDIA CUDA C code via PyCUDA\"\n",
    "    notebook_path = os.path.abspath(\"\")\n",
    "    mod = SourceModule(cell)\n",
    "    run_me = mod.get_function(\"run_me\")\n",
    "    run_me(block=(4, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world\n",
      "hello world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "__global__ void run_me()\n",
    "{\n",
    "    printf(\"hello world\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cuda test_kernel arg1 arg2 block=(1,2,3) grid=(4,5,6)\n",
    "# requires parsing ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no syntax highlighting in magic cells :(\n",
    "\n",
    "maybe a custom CUDA Kernel?\n",
    "\n",
    "https://jupyter-client.readthedocs.io/en/latest/kernels.html\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/install/kernel_install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CudaSourceFile\n",
    "Instead of going the Jupyter route, I'll just use external files for all the IDE\n",
    "goodness. Also enables reusability like headerfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0, 0): Block (1, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(5, 0, 0): Block (1, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(6, 0, 0): Block (1, 0, 0), Thread (2, 0, 0) -- 42\n",
      "(7, 0, 0): Block (1, 0, 0), Thread (3, 0, 0) -- 42\n",
      "(4, 1, 0): Block (1, 0, 0), Thread (0, 1, 0) -- 42\n",
      "(5, 1, 0): Block (1, 0, 0), Thread (1, 1, 0) -- 42\n",
      "(6, 1, 0): Block (1, 0, 0), Thread (2, 1, 0) -- 42\n",
      "(7, 1, 0): Block (1, 0, 0), Thread (3, 1, 0) -- 42\n",
      "(4, 0, 1): Block (1, 0, 0), Thread (0, 0, 1) -- 42\n",
      "(5, 0, 1): Block (1, 0, 0), Thread (1, 0, 1) -- 42\n",
      "(6, 0, 1): Block (1, 0, 0), Thread (2, 0, 1) -- 42\n",
      "(7, 0, 1): Block (1, 0, 0), Thread (3, 0, 1) -- 42\n",
      "(4, 1, 1): Block (1, 0, 0), Thread (0, 1, 1) -- 42\n",
      "(5, 1, 1): Block (1, 0, 0), Thread (1, 1, 1) -- 42\n",
      "(6, 1, 1): Block (1, 0, 0), Thread (2, 1, 1) -- 42\n",
      "(7, 1, 1): Block (1, 0, 0), Thread (3, 1, 1) -- 42\n",
      "(0, 0, 0): Block (0, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(1, 0, 0): Block (0, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(2, 0, 0): Block (0, 0, 0), Thread (2, 0, 0) -- 42\n",
      "(3, 0, 0): Block (0, 0, 0), Thread (3, 0, 0) -- 42\n",
      "(0, 1, 0): Block (0, 0, 0), Thread (0, 1, 0) -- 42\n",
      "(1, 1, 0): Block (0, 0, 0), Thread (1, 1, 0) -- 42\n",
      "(2, 1, 0): Block (0, 0, 0), Thread (2, 1, 0) -- 42\n",
      "(3, 1, 0): Block (0, 0, 0), Thread (3, 1, 0) -- 42\n",
      "(0, 0, 1): Block (0, 0, 0), Thread (0, 0, 1) -- 42\n",
      "(1, 0, 1): Block (0, 0, 0), Thread (1, 0, 1) -- 42\n",
      "(2, 0, 1): Block (0, 0, 0), Thread (2, 0, 1) -- 42\n",
      "(3, 0, 1): Block (0, 0, 0), Thread (3, 0, 1) -- 42\n",
      "(0, 1, 1): Block (0, 0, 0), Thread (0, 1, 1) -- 42\n",
      "(1, 1, 1): Block (0, 0, 0), Thread (1, 1, 1) -- 42\n",
      "(2, 1, 1): Block (0, 0, 0), Thread (2, 1, 1) -- 42\n",
      "(3, 1, 1): Block (0, 0, 0), Thread (3, 1, 1) -- 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "\n",
    "class CudaSourceFile:\n",
    "    def __init__(\n",
    "        self, filename: str, kernels: list[str] = list(), include_dirs: list[str] = list()\n",
    "    ) -> None:\n",
    "        with open(filename) as f:\n",
    "            file_str = f.read()\n",
    "            self.mod = SourceModule(file_str, include_dirs=include_dirs)\n",
    "\n",
    "        for k in kernels:\n",
    "            setattr(self, k, self.mod.get_function(k))\n",
    "\n",
    "\n",
    "cf = CudaSourceFile(\"test_kernel.cu\", kernels=[\"test_kernel\"], include_dirs=[notebook_path])\n",
    "cf.test_kernel(block=(4, 2, 2), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Transfers\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations\n",
    "> Memory optimizations are the most important area for performance.\n",
    "\n",
    "> it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.\n",
    "\n",
    "> Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.\n",
    "\n",
    "> higher bandwidth between the host and the device is achieved when using page-locked (or pinned) memory\n",
    "\n",
    "---\n",
    "\n",
    "Some useful hints:\n",
    "- https://medium.com/@rupertt/accelerate-computation-with-pycuda-2c12a6555cc6\n",
    "  - cuda.memcpy_htod\n",
    "  - cuda.memcpy_dtoh\n",
    "- https://wlandau.github.io/gpu/lectures/pycuda/pycuda.pdf\n",
    "  - In, Out, InOut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [   0  256  512  768 1024 1280 1536 1792    1  257  513  769 1025 1281\n",
      " 1537 1793    2  258  514  770 1026 1282 1538 1794    3  259  515  771\n",
      " 1027 1283 1539 1795]\n",
      "npout [   0  256  512  768 1024 1280 1536 1792    1  257  513  769 1025 1281\n",
      " 1537 1793    2  258  514  770 1026 1282 1538 1794    3  259  515  771\n",
      " 1027 1283 1539 1795]\n",
      "npout[10] 513\n",
      "npout pair (2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0): out[0] = 0\n",
      "(1, 0): out[1] = 256\n",
      "(2, 0): out[2] = 512\n",
      "(3, 0): out[3] = 768\n",
      "(4, 0): out[4] = 1024\n",
      "(5, 0): out[5] = 1280\n",
      "(6, 0): out[6] = 1536\n",
      "(7, 0): out[7] = 1792\n",
      "(0, 1): out[8] = 1\n",
      "(1, 1): out[9] = 257\n",
      "(2, 1): out[10] = 513\n",
      "(3, 1): out[11] = 769\n",
      "(4, 1): out[12] = 1025\n",
      "(5, 1): out[13] = 1281\n",
      "(6, 1): out[14] = 1537\n",
      "(7, 1): out[15] = 1793\n",
      "(0, 2): out[16] = 2\n",
      "(1, 2): out[17] = 258\n",
      "(2, 2): out[18] = 514\n",
      "(3, 2): out[19] = 770\n",
      "(4, 2): out[20] = 1026\n",
      "(5, 2): out[21] = 1282\n",
      "(6, 2): out[22] = 1538\n",
      "(7, 2): out[23] = 1794\n",
      "(0, 3): out[24] = 3\n",
      "(1, 3): out[25] = 259\n",
      "(2, 3): out[26] = 515\n",
      "(3, 3): out[27] = 771\n",
      "(4, 3): out[28] = 1027\n",
      "(5, 3): out[29] = 1283\n",
      "(6, 3): out[30] = 1539\n",
      "(7, 3): out[31] = 1795\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_pair(n):\n",
    "    return (n >> 8, n & 0xFF)\n",
    "\n",
    "\n",
    "cf = CudaSourceFile(\"test_indicies.cu\", kernels=[\"test_indicies\"], include_dirs=[notebook_path])\n",
    "\n",
    "cols = 8\n",
    "rows = 4\n",
    "out = cuda.managed_empty(shape=rows * cols, dtype=np.int32, mem_flags=cuda.mem_attach_flags.GLOBAL)\n",
    "cf.test_indicies(np.int32(cols), out, block=(8, 4, 1), grid=(1, 1, 1))\n",
    "print(\"out:\", out)\n",
    "npout = np.array(out)\n",
    "print(\"npout\", npout)\n",
    "print(\"npout[10]\", npout[10])\n",
    "print(\"npout pair\", to_pair(npout[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## managed_empty\n",
    "\n",
    "https://documen.tician.de/pycuda/driver.html#managed-memory\n",
    "\n",
    "> CUDA 6.0 adds support for a “Unified Memory” model, which creates a managed virtual memory space that is visible to both CPUs and GPUs. The OS will migrate the physical pages associated with managed memory between the CPU and GPU as needed. This allows a numpy array on the host to be passed to kernels without first creating a DeviceAllocation and manually copying the host data to and from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "\n",
    "out = cuda.managed_empty(shape=(3, 2), dtype=np.int32, mem_flags=cuda.mem_attach_flags.GLOBAL)\n",
    "cf.test_pairs(out, block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"out:\", out)\n",
    "npout = np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## malloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npout [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n",
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "rows = 3\n",
    "cols = 2\n",
    "sizeof_int = 4\n",
    "out = cuda.mem_alloc(rows * cols * sizeof_int)\n",
    "cf.test_pairs(out, block=(3, 1, 1), grid=(1, 1, 1))\n",
    "npout = np.empty((3, 2), dtype=np.int32)\n",
    "cuda.memcpy_dtoh(npout, out)\n",
    "print(\"npout\", npout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pycuda.Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n",
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "res = np.empty((3, 2), dtype=np.int32)\n",
    "cf.test_pairs(cuda.Out(res), block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"res:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from_device / device allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "res = np.empty((3, 2), dtype=np.int32)\n",
    "cf.test_pairs(cuda.Out(res), block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"res:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pycuda.In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buf [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "(0, 0): val 0\n",
      "(1, 0): val 1\n",
      "(2, 0): val 2\n",
      "(0, 1): val 3\n",
      "(1, 1): val 4\n",
      "(2, 1): val 5\n",
      "(0, 2): val 6\n",
      "(1, 2): val 7\n",
      "(2, 2): val 8\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"dump_buf.cu\", kernels=[\"dump_buf\"], include_dirs=[notebook_path])\n",
    "\n",
    "input_buf = np.arange(12).reshape((3, 4)).astype(np.int8)\n",
    "width = 3\n",
    "height = 2\n",
    "cf.dump_buf(np.int32(width), np.int32(height), cuda.In(input_buf), block=(3, 3, 1))\n",
    "cuda.context.synchronize()\n",
    "print(\"input_buf\", input_buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- [ ] from_device\n",
    "  - https://documen.tician.de/pycuda/driver.html#pycuda.driver.from_device\n",
    "- [ ] mem_alloc_pitch\n",
    "  - https://documen.tician.de/pycuda/driver.html#pycuda.driver.mem_alloc_pitch\n",
    "- [ ] page-locked host memory\n",
    "  - https://documen.tician.de/pycuda/driver.html#pagelocked-host-memory\n",
    "  - https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory\n",
    "- [ ] memory pools\n",
    "  - https://documen.tician.de/pycuda/util.html#device-based-memory-pool\n",
    "- [ ] structs\n",
    "  - https://github.com/minrk/PyCUDA/blob/master/doc/source/tutorial.rst#structures\n",
    "- [ ] Shared memory\n",
    "  - https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daisy Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "nvcc compilation of /tmp/tmp_kjz3mqj/kernel.cu failed\n[command: nvcc --cubin -arch sm_89 -I/home/apowers/Projects/roc/experiments/2024.08.30-07.27.46-pycuda-learning -I/home/apowers/Projects/roc/.venv/lib/python3.11/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nptxas fatal   : Unresolved extern function '__cudaCDP2EventCreateWithFlags'\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gpuarray\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m cf \u001b[38;5;241m=\u001b[39m \u001b[43mCudaSourceFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdaisy_chain.cu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnotebook_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m e \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mEvent()\n\u001b[1;32m     31\u001b[0m cf\u001b[38;5;241m.\u001b[39mk1(e, block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mCudaSourceFile.__init__\u001b[0;34m(self, filename, kernels, include_dirs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m     file_str \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmod \u001b[38;5;241m=\u001b[39m \u001b[43mSourceModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# options=[\"-rdc=true\"],\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kernels:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmod\u001b[38;5;241m.\u001b[39mget_function(k))\n",
      "File \u001b[0;32m~/Projects/roc/.venv/lib/python3.11/site-packages/pycuda/compiler.py:348\u001b[0m, in \u001b[0;36mSourceModule.__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    336\u001b[0m     source,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     include_dirs\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    345\u001b[0m ):\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_arch(arch)\n\u001b[0;32m--> 348\u001b[0m     cubin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_extern_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43march\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_dirs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_from_buffer\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m module_from_buffer(cubin)\n",
      "File \u001b[0;32m~/Projects/roc/.venv/lib/python3.11/site-packages/pycuda/compiler.py:297\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs, target)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m include_dirs:\n\u001b[1;32m    295\u001b[0m     options\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m i)\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/roc/.venv/lib/python3.11/site-packages/pycuda/compiler.py:154\u001b[0m, in \u001b[0;36mcompile_plain\u001b[0;34m(source, options, keep, nvcc, cache_dir, target)\u001b[0m\n\u001b[1;32m    148\u001b[0m         warn(\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyCUDA: nvcc exited with status 0, but appears to have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencountered an error\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileError\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompileError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvcc compilation of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m cu_file_path,\n\u001b[1;32m    156\u001b[0m         cmdline,\n\u001b[1;32m    157\u001b[0m         stdout\u001b[38;5;241m=\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    158\u001b[0m         stderr\u001b[38;5;241m=\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout \u001b[38;5;129;01mor\u001b[39;00m stderr:\n\u001b[1;32m    162\u001b[0m     lcase_err_text \u001b[38;5;241m=\u001b[39m (stdout \u001b[38;5;241m+\u001b[39m stderr)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[0;31mCompileError\u001b[0m: nvcc compilation of /tmp/tmp_kjz3mqj/kernel.cu failed\n[command: nvcc --cubin -arch sm_89 -I/home/apowers/Projects/roc/experiments/2024.08.30-07.27.46-pycuda-learning -I/home/apowers/Projects/roc/.venv/lib/python3.11/site-packages/pycuda/cuda kernel.cu]\n[stderr:\nptxas fatal   : Unresolved extern function '__cudaCDP2EventCreateWithFlags'\n]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "\n",
    "class CudaSourceFile:\n",
    "    def __init__(\n",
    "        self, filename: str, kernels: list[str] = list(), include_dirs: list[str] = list()\n",
    "    ) -> None:\n",
    "        with open(filename) as f:\n",
    "            file_str = f.read()\n",
    "            self.mod = SourceModule(\n",
    "                file_str,\n",
    "                include_dirs=include_dirs,\n",
    "                # options=[\"-rdc=true\"],\n",
    "            )\n",
    "\n",
    "        for k in kernels:\n",
    "            setattr(self, k, self.mod.get_function(k))\n",
    "\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"daisy_chain.cu\", kernels=[\"k1\", \"k2\"], include_dirs=[notebook_path])\n",
    "e = cuda.Event()\n",
    "cf.k1(e, block=(2, 1, 1))\n",
    "cf.k2(e, block=(2, 1, 1))\n",
    "\n",
    "# grid = np.array(\n",
    "#     [\n",
    "#         [3, 3, 0, 5],\n",
    "#         [0, 1, 0, 5],\n",
    "#         [0, 0, 4, 2],\n",
    "#     ],\n",
    "#     dtype=np.int16,\n",
    "# )\n",
    "# print(f\"grid:\\n{grid}\")\n",
    "\n",
    "# res = np.zeros_like(grid)\n",
    "\n",
    "# width = grid.shape[0]\n",
    "# height = grid.shape[1]\n",
    "# np_width = np.int32(width)\n",
    "# np_height = np.int32(height)\n",
    "\n",
    "# cf.fe_single(cuda.Out(res), np_width, np_height, cuda.In(grid), block=(width, height, 1))\n",
    "# print(f\"res:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Python\n",
    "\n",
    "NVIDIA's version of PyCUDA supports graphs for streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "Copied from https://nvidia.github.io/cuda-python/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log size is 159\n",
      "Log: saxpy.cu(5): warning #177-D: variable \"xxx\" was declared but never referenced\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "from cuda import cuda, nvrtc\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def _cudaGetErrorEnum(error):\n",
    "    if isinstance(error, cuda.CUresult):\n",
    "        err, name = cuda.cuGetErrorName(error)\n",
    "        return name if err == cuda.CUresult.CUDA_SUCCESS else \"<unknown>\"\n",
    "    elif isinstance(error, nvrtc.nvrtcResult):\n",
    "        return nvrtc.nvrtcGetErrorString(error)[1]\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown error type: {}\".format(error))\n",
    "\n",
    "\n",
    "def checkCudaErrors(result):\n",
    "    # if result[0].value and _cudaGetErrorEnum(result[0]) == NVRTC_ERROR_COMPILATION\n",
    "    if result[0].value:\n",
    "        print(\"ERR STRING\", result)\n",
    "        raise RuntimeError(\n",
    "            \"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0]))\n",
    "        )\n",
    "    if len(result) == 1:\n",
    "        return None\n",
    "    elif len(result) == 2:\n",
    "        return result[1]\n",
    "    else:\n",
    "        return result[1:]\n",
    "\n",
    "\n",
    "with open(\"saxpy.cu\") as f:\n",
    "    saxpy = f.read()\n",
    "\n",
    "# Initialize CUDA Driver API\n",
    "checkCudaErrors(cuda.cuInit(0))\n",
    "\n",
    "# Retrieve handle for device 0\n",
    "cuDevice = checkCudaErrors(cuda.cuDeviceGet(0))\n",
    "\n",
    "# Derive target architecture for device 0\n",
    "major = checkCudaErrors(\n",
    "    cuda.cuDeviceGetAttribute(\n",
    "        cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cuDevice\n",
    "    )\n",
    ")\n",
    "minor = checkCudaErrors(\n",
    "    cuda.cuDeviceGetAttribute(\n",
    "        cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cuDevice\n",
    "    )\n",
    ")\n",
    "arch_arg = bytes(f\"--gpu-architecture=compute_{major}{minor}\", \"ascii\")\n",
    "\n",
    "# Create program\n",
    "prog = checkCudaErrors(nvrtc.nvrtcCreateProgram(str.encode(saxpy), b\"saxpy.cu\", 0, [], []))\n",
    "\n",
    "# Compile program\n",
    "opts = [b\"--fmad=false\", arch_arg]\n",
    "ret = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "# ret = nvrtc.nvrtcCompileProgram(prog, 0, [])\n",
    "log_sz = checkCudaErrors(nvrtc.nvrtcGetProgramLogSize(prog))\n",
    "print(\"Log size is\", log_sz)\n",
    "buf = b\" \" * log_sz\n",
    "checkCudaErrors(nvrtc.nvrtcGetProgramLog(prog, buf))\n",
    "print(\"Log:\", buf.decode())\n",
    "checkCudaErrors(ret)\n",
    "\n",
    "# Get PTX from compilation\n",
    "ptxSize = checkCudaErrors(nvrtc.nvrtcGetPTXSize(prog))\n",
    "ptx = b\" \" * ptxSize\n",
    "checkCudaErrors(nvrtc.nvrtcGetPTX(prog, ptx))\n",
    "\n",
    "# Create context\n",
    "context = checkCudaErrors(cuda.cuCtxCreate(0, cuDevice))\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "ptx = np.char.array(ptx)\n",
    "# Note: Incompatible --gpu-architecture would be detected here\n",
    "module = checkCudaErrors(cuda.cuModuleLoadData(ptx.ctypes.data))\n",
    "kernel = checkCudaErrors(cuda.cuModuleGetFunction(module, b\"saxpy\"))\n",
    "\n",
    "NUM_THREADS = 512  # Threads per block\n",
    "NUM_BLOCKS = 32768  # Blocks per grid\n",
    "\n",
    "a = np.array([2.0], dtype=np.float32)\n",
    "n = np.array(NUM_THREADS * NUM_BLOCKS, dtype=np.uint32)\n",
    "bufferSize = n * a.itemsize\n",
    "\n",
    "hX = np.random.rand(n).astype(dtype=np.float32)\n",
    "hY = np.random.rand(n).astype(dtype=np.float32)\n",
    "hOut = np.zeros(n).astype(dtype=np.float32)\n",
    "\n",
    "dXclass = checkCudaErrors(cuda.cuMemAlloc(bufferSize))\n",
    "dYclass = checkCudaErrors(cuda.cuMemAlloc(bufferSize))\n",
    "dOutclass = checkCudaErrors(cuda.cuMemAlloc(bufferSize))\n",
    "\n",
    "stream = checkCudaErrors(cuda.cuStreamCreate(0))\n",
    "\n",
    "checkCudaErrors(cuda.cuMemcpyHtoDAsync(dXclass, hX.ctypes.data, bufferSize, stream))\n",
    "checkCudaErrors(cuda.cuMemcpyHtoDAsync(dYclass, hY.ctypes.data, bufferSize, stream))\n",
    "\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "dX = np.array([int(dXclass)], dtype=np.uint64)\n",
    "dY = np.array([int(dYclass)], dtype=np.uint64)\n",
    "dOut = np.array([int(dOutclass)], dtype=np.uint64)\n",
    "\n",
    "args = [a, dX, dY, dOut, n]\n",
    "args = np.array([arg.ctypes.data for arg in args], dtype=np.uint64)\n",
    "\n",
    "checkCudaErrors(\n",
    "    cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        NUM_BLOCKS,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        NUM_THREADS,  # block x dim\n",
    "        1,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        stream,  # stream\n",
    "        args.ctypes.data,  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    ")\n",
    "\n",
    "checkCudaErrors(cuda.cuMemcpyDtoHAsync(hOut.ctypes.data, dOutclass, bufferSize, stream))\n",
    "checkCudaErrors(cuda.cuStreamSynchronize(stream))\n",
    "\n",
    "# Assert values are same after running kernel\n",
    "hZ = a * hX + hY\n",
    "if not np.allclose(hOut, hZ):\n",
    "    raise ValueError(\"Error outside tolerance for host-device vectors\")\n",
    "\n",
    "checkCudaErrors(cuda.cuStreamDestroy(stream))\n",
    "checkCudaErrors(cuda.cuMemFree(dXclass))\n",
    "checkCudaErrors(cuda.cuMemFree(dYclass))\n",
    "checkCudaErrors(cuda.cuMemFree(dOutclass))\n",
    "checkCudaErrors(cuda.cuModuleUnload(module))\n",
    "checkCudaErrors(cuda.cuCtxDestroy(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE:\n",
      "-------\n",
      "extern \"C\" {\n",
      "#include \"test.h\"\n",
      "\n",
      "__global__ void test_kernel() {\n",
      "  printf(\"(%d, %d, %d): Block (%d, %d, %d), Thread (%d, %d, %d) -- %d\\n\", MY_X,\n",
      "         MY_Y, MY_Z, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x,\n",
      "         threadIdx.y, threadIdx.z, MY_THING);\n",
      "}\n",
      "}\n",
      "\n",
      "-------\n",
      "\n",
      "Log size is 1\n",
      "Log: \u0000\n",
      "(2, 0, 0): Block (1, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(3, 0, 0): Block (1, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(0, 0, 0): Block (0, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(1, 0, 0): Block (0, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(4, 0, 0): Block (2, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(5, 0, 0): Block (2, 0, 0), Thread (1, 0, 0) -- 42\n"
     ]
    }
   ],
   "source": [
    "from cuda import cuda, nvrtc\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def _cudaGetErrorEnum(error):\n",
    "    if isinstance(error, cuda.CUresult):\n",
    "        err, name = cuda.cuGetErrorName(error)\n",
    "        return name if err == cuda.CUresult.CUDA_SUCCESS else \"<unknown>\"\n",
    "    elif isinstance(error, nvrtc.nvrtcResult):\n",
    "        return nvrtc.nvrtcGetErrorString(error)[1]\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown error type: {}\".format(error))\n",
    "\n",
    "\n",
    "def checkCudaErrors(result):\n",
    "    # if result[0].value and _cudaGetErrorEnum(result[0]) == NVRTC_ERROR_COMPILATION\n",
    "    if result[0].value:\n",
    "        print(\"ERR STRING\", result)\n",
    "        raise RuntimeError(\n",
    "            \"CUDA error code={}({})\".format(result[0].value, _cudaGetErrorEnum(result[0]))\n",
    "        )\n",
    "    if len(result) == 1:\n",
    "        return None\n",
    "    elif len(result) == 2:\n",
    "        return result[1]\n",
    "    else:\n",
    "        return result[1:]\n",
    "\n",
    "\n",
    "with open(\"test_kernel.cu\") as f:\n",
    "    code = 'extern \"C\" {\\n' + f.read() + \"\\n}\\n\"\n",
    "print(f\"CODE:\\n-------\\n{code}\\n-------\\n\")\n",
    "\n",
    "# def _find_cuda_incl_path() -> pathlib.Path:\n",
    "#     \"Tries to find the CUDA include path.\"\n",
    "#     cuda_path = os.getenv(\"CUDA_PATH\")\n",
    "#     if cuda_path is None:\n",
    "#         if sys.platform == \"linux\":\n",
    "#             cuda_path = pathlib.Path(\"/usr/local/cuda/include\")\n",
    "#             if not (cuda_path.exists() and cuda_path.is_dir()):\n",
    "#                 cuda_path = None\n",
    "#         elif sys.platform == \"win32\":\n",
    "#             ...\n",
    "#         elif sys.platform == \"darwin\":\n",
    "#             ...\n",
    "#     else:\n",
    "#         cuda_path = pathlib.Path(cuda_path)\n",
    "#         cuda_path /= \"include\"\n",
    "\n",
    "#     return cuda_path\n",
    "\n",
    "\n",
    "# CUDA_HOME = os.getenv(\"CUDA_HOME\")\n",
    "# if CUDA_HOME == None:\n",
    "#     CUDA_HOME = os.getenv(\"CUDA_PATH\")\n",
    "# if CUDA_HOME == None:\n",
    "#     raise RuntimeError(\"Environment variable CUDA_HOME or CUDA_PATH is not set\")\n",
    "# include_dirs = os.path.join(CUDA_HOME, \"include\")\n",
    "\n",
    "# Initialize CUDA Driver API\n",
    "checkCudaErrors(cuda.cuInit(0))\n",
    "\n",
    "# Retrieve handle for device 0\n",
    "cuDevice = checkCudaErrors(cuda.cuDeviceGet(0))\n",
    "\n",
    "# Derive target architecture for device 0\n",
    "major = checkCudaErrors(\n",
    "    cuda.cuDeviceGetAttribute(\n",
    "        cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, cuDevice\n",
    "    )\n",
    ")\n",
    "minor = checkCudaErrors(\n",
    "    cuda.cuDeviceGetAttribute(\n",
    "        cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, cuDevice\n",
    "    )\n",
    ")\n",
    "arch_arg = bytes(f\"--gpu-architecture=compute_{major}{minor}\", \"ascii\")\n",
    "\n",
    "# Create program\n",
    "prog = checkCudaErrors(nvrtc.nvrtcCreateProgram(str.encode(code), b\"test_kernel.cu\", 0, [], []))\n",
    "\n",
    "# Compile program\n",
    "opts = [b\"--fmad=false\", arch_arg]\n",
    "# ret = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "ret = nvrtc.nvrtcCompileProgram(prog, 0, [])\n",
    "log_sz = checkCudaErrors(nvrtc.nvrtcGetProgramLogSize(prog))\n",
    "print(\"Log size is\", log_sz)\n",
    "buf = b\" \" * log_sz\n",
    "checkCudaErrors(nvrtc.nvrtcGetProgramLog(prog, buf))\n",
    "print(\"Log:\", buf.decode())\n",
    "checkCudaErrors(ret)\n",
    "\n",
    "# Get PTX from compilation\n",
    "ptxSize = checkCudaErrors(nvrtc.nvrtcGetPTXSize(prog))\n",
    "ptx = b\" \" * ptxSize\n",
    "checkCudaErrors(nvrtc.nvrtcGetPTX(prog, ptx))\n",
    "\n",
    "# Create context\n",
    "context = checkCudaErrors(cuda.cuCtxCreate(0, cuDevice))\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "ptx = np.char.array(ptx)\n",
    "module = checkCudaErrors(cuda.cuModuleLoadData(ptx.ctypes.data))\n",
    "kernel = checkCudaErrors(cuda.cuModuleGetFunction(module, b\"test_kernel\"))\n",
    "stream = checkCudaErrors(cuda.cuStreamCreate(0))\n",
    "\n",
    "# args = np.array([], np.uint64)\n",
    "\n",
    "checkCudaErrors(\n",
    "    cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        3,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        2,  # block x dim\n",
    "        1,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        stream,  # stream\n",
    "        #    args.ctypes.data,  # kernel arguments\n",
    "        0,  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    ")\n",
    "\n",
    "checkCudaErrors(cuda.cuStreamSynchronize(stream))\n",
    "checkCudaErrors(cuda.cuStreamDestroy(stream))\n",
    "checkCudaErrors(cuda.cuModuleUnload(module))\n",
    "checkCudaErrors(cuda.cuCtxDestroy(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid:\n",
      "[[3 3 0 5]\n",
      " [0 1 0 5]\n",
      " [0 0 4 2]]\n",
      "(0, 0) val: 3, unique 0\n",
      "(1, 0) val: 3, unique 0\n",
      "(2, 0) val: 0, unique 0\n",
      "(0, 1) val: 5, unique 0\n",
      "(1, 1) val: 0, unique 0\n",
      "(2, 1) val: 1, unique 1\n",
      "(0, 2) val: 0, unique 0\n",
      "(1, 2) val: 5, unique 0\n",
      "(2, 2) val: 0, unique 0\n",
      "(0, 3) val: 0, unique 0\n",
      "(1, 3) val: 4, unique 1\n",
      "(2, 3) val: 2, unique 1\n",
      "res:\n",
      "[[0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"fe_single.cu\", kernels=[\"fe_single\"], include_dirs=[notebook_path])\n",
    "\n",
    "grid = np.array(\n",
    "    [\n",
    "        [3, 3, 0, 5],\n",
    "        [0, 1, 0, 5],\n",
    "        [0, 0, 4, 2],\n",
    "    ],\n",
    "    dtype=np.int16,\n",
    ")\n",
    "print(f\"grid:\\n{grid}\")\n",
    "\n",
    "res = np.zeros_like(grid)\n",
    "\n",
    "width = grid.shape[0]\n",
    "height = grid.shape[1]\n",
    "np_width = np.int32(width)\n",
    "np_height = np.int32(height)\n",
    "\n",
    "cf.fe_single(cuda.Out(res), np_width, np_height, cuda.In(grid), block=(width, height, 1))\n",
    "print(f\"res:\\n{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://documen.tician.de/pycuda/driver.html#profiler-control\n",
    "\n",
    "`driver.initialize_profiler(config_file, output_file, output_mode)`\n",
    "\n",
    "`driver.start_profiler()`\n",
    "\n",
    "`driver.stop_profiler()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
