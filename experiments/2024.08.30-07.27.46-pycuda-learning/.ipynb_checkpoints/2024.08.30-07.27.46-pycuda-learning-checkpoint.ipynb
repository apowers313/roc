{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[pycuda learning](https://github.com/apowers313/roc/blob/master/experiments/2024.08.30-07.27.46-pycuda-learning/2024.08.30-07.27.46-pycuda-learning.ipynb)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 31 15:35:38 PDT 2024\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/usr/local/cuda/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":/usr/local/cuda/bin\"\n",
    "print(\"PATH:\", os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  2 12:12:12 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   50C    P8              4W /  220W |       2MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Mon_Oct_24_19:12:58_PDT_2022\n",
      "Cuda compilation tools, release 12.0, V12.0.76\n",
      "Build cuda_12.0.r12.0/compiler.31968024_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 CUDA Capable device(s)\n",
      "Device NVIDIA GeForce RTX 4070 SUPER:\n",
      "\tCompute Capability: 8.9\n",
      "\tTotal Memory: 12002\n",
      "\t(56) Multiprocessors, (64) CUDA Cores / Multiprocessor: 3584 CUDA Cores\n",
      "\tASYNC_ENGINE_COUNT: 2\n",
      "\tCAN_MAP_HOST_MEMORY: 1\n",
      "\tCAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 1\n",
      "\tCLOCK_RATE: 2475000\n",
      "\tCOMPUTE_CAPABILITY_MAJOR: 8\n",
      "\tCOMPUTE_CAPABILITY_MINOR: 9\n",
      "\tCOMPUTE_MODE: DEFAULT\n",
      "\tCOMPUTE_PREEMPTION_SUPPORTED: 1\n",
      "\tCONCURRENT_KERNELS: 1\n",
      "\tCONCURRENT_MANAGED_ACCESS: 1\n",
      "\tDIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0\n",
      "\tECC_ENABLED: 0\n",
      "\tGENERIC_COMPRESSION_SUPPORTED: 1\n",
      "\tGLOBAL_L1_CACHE_SUPPORTED: 1\n",
      "\tGLOBAL_MEMORY_BUS_WIDTH: 192\n",
      "\tGPU_OVERLAP: 1\n",
      "\tHANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1\n",
      "\tHANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0\n",
      "\tHANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0\n",
      "\tHOST_NATIVE_ATOMIC_SUPPORTED: 0\n",
      "\tINTEGRATED: 0\n",
      "\tKERNEL_EXEC_TIMEOUT: 0\n",
      "\tL2_CACHE_SIZE: 50331648\n",
      "\tLOCAL_L1_CACHE_SUPPORTED: 1\n",
      "\tMANAGED_MEMORY: 1\n",
      "\tMAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE1D_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACE2D_WIDTH: 131072\n",
      "\tMAXIMUM_SURFACE3D_DEPTH: 16384\n",
      "\tMAXIMUM_SURFACE3D_HEIGHT: 16384\n",
      "\tMAXIMUM_SURFACE3D_WIDTH: 16384\n",
      "\tMAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\tMAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "\tMAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
      "\tMAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE1D_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "\tMAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
      "\tMAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
      "\tMAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURE2D_WIDTH: 131072\n",
      "\tMAXIMUM_TEXTURE3D_DEPTH: 16384\n",
      "\tMAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
      "\tMAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
      "\tMAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
      "\tMAXIMUM_TEXTURE3D_WIDTH: 16384\n",
      "\tMAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
      "\tMAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "\tMAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
      "\tMAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
      "\tMAX_BLOCKS_PER_MULTIPROCESSOR: 24\n",
      "\tMAX_BLOCK_DIM_X: 1024\n",
      "\tMAX_BLOCK_DIM_Y: 1024\n",
      "\tMAX_BLOCK_DIM_Z: 64\n",
      "\tMAX_GRID_DIM_X: 2147483647\n",
      "\tMAX_GRID_DIM_Y: 65535\n",
      "\tMAX_GRID_DIM_Z: 65535\n",
      "\tMAX_PERSISTING_L2_CACHE_SIZE: 34603008\n",
      "\tMAX_PITCH: 2147483647\n",
      "\tMAX_REGISTERS_PER_BLOCK: 65536\n",
      "\tMAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "\tMAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "\tMAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 101376\n",
      "\tMAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 102400\n",
      "\tMAX_THREADS_PER_BLOCK: 1024\n",
      "\tMAX_THREADS_PER_MULTIPROCESSOR: 1536\n",
      "\tMEMORY_CLOCK_RATE: 10501000\n",
      "\tMEMORY_POOLS_SUPPORTED: 1\n",
      "\tMULTI_GPU_BOARD: 0\n",
      "\tMULTI_GPU_BOARD_GROUP_ID: 0\n",
      "\tPAGEABLE_MEMORY_ACCESS: 0\n",
      "\tPAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0\n",
      "\tPCI_BUS_ID: 1\n",
      "\tPCI_DEVICE_ID: 0\n",
      "\tPCI_DOMAIN_ID: 0\n",
      "\tREAD_ONLY_HOST_REGISTER_SUPPORTED: 1\n",
      "\tRESERVED_SHARED_MEMORY_PER_BLOCK: 1024\n",
      "\tSINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 64\n",
      "\tSTREAM_PRIORITIES_SUPPORTED: 1\n",
      "\tSURFACE_ALIGNMENT: 512\n",
      "\tTCC_DRIVER: 0\n",
      "\tTEXTURE_ALIGNMENT: 512\n",
      "\tTEXTURE_PITCH_ALIGNMENT: 32\n",
      "\tTOTAL_CONSTANT_MEMORY: 65536\n",
      "\tUNIFIED_ADDRESSING: 1\n",
      "\tWARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/Chapter03/deviceQuery.py\n",
    "\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "\n",
    "print(f\"Detected {drv.Device.count()} CUDA Capable device(s)\")\n",
    "\n",
    "for i in range(drv.Device.count()):\n",
    "    gpu_device = drv.Device(i)\n",
    "    print(f\"Device {gpu_device.name()}:\")\n",
    "    compute_capability = float(\"%d.%d\" % gpu_device.compute_capability())\n",
    "    print(f\"\\tCompute Capability: {compute_capability}\")\n",
    "    print(f\"\\tTotal Memory: {gpu_device.total_memory()//(1024**2)}\")\n",
    "\n",
    "    raw_device_attributes = gpu_device.get_attributes()\n",
    "    device_attributes = {str(k): raw_device_attributes[k] for k in raw_device_attributes.keys()}\n",
    "\n",
    "    num_mp = device_attributes[\"MULTIPROCESSOR_COUNT\"]\n",
    "\n",
    "    major_compute_capability = gpu_device.compute_capability()[0]\n",
    "    cuda_cores_per_mp = {\n",
    "        # Maxwell\n",
    "        5.0: 128,\n",
    "        5.1: 128,\n",
    "        5.2: 128,\n",
    "        # Pascal\n",
    "        6.0: 64,\n",
    "        6.1: 128,\n",
    "        6.2: 128,\n",
    "        # Volta and Turing\n",
    "        7.0: 64,\n",
    "        7.5: 64,\n",
    "        # Ampere\n",
    "        8.0: 64,\n",
    "        8.6: 128,\n",
    "        8.9: 128,  # Ada Lovelace\n",
    "        # Hopper\n",
    "        9.0: 128,\n",
    "    }[major_compute_capability]\n",
    "\n",
    "    # RTX 4070 SUPER should have 7168 CUDA Cores, this reports 3584 which isn't right\n",
    "    # https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4070-family/\n",
    "    print(\n",
    "        f\"\\t({num_mp}) Multiprocessors, ({cuda_cores_per_mp}) CUDA Cores / Multiprocessor: {num_mp*cuda_cores_per_mp} CUDA Cores\"\n",
    "    )\n",
    "\n",
    "    device_attributes.pop(\"MULTIPROCESSOR_COUNT\")\n",
    "\n",
    "    for k in device_attributes.keys():\n",
    "        print(f\"\\t{k}: {device_attributes[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "\n",
    "host_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "device_data = gpuarray.to_gpu(host_data)\n",
    "device_data_x2 = 2 * device_data\n",
    "host_data_x2 = device_data_x2.get()\n",
    "print(host_data_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from thread 0, in block 1!\n",
      "Hello world from thread 1, in block 1!\n",
      "Hello world from thread 2, in block 1!\n",
      "Hello world from thread 3, in block 1!\n",
      "Hello world from thread 4, in block 1!\n",
      "Hello world from thread 0, in block 0!\n",
      "Hello world from thread 1, in block 0!\n",
      "Hello world from thread 2, in block 0!\n",
      "Hello world from thread 3, in block 0!\n",
      "Hello world from thread 4, in block 0!\n",
      "-------------------------------------\n",
      "This kernel was launched over a grid consisting of 2 blocks,\n",
      "where each block has 5 threads.\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ker = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void hello_world_ker()\n",
    "{\n",
    "\tprintf(\"Hello world from thread %d, in block %d!\\\\n\", threadIdx.x, blockIdx.x);\n",
    "\t\n",
    "\t__syncthreads();\n",
    "\t\n",
    "\tif(threadIdx.x == 0 && blockIdx.x == 0)\n",
    "\t{\n",
    "\t\tprintf(\"-------------------------------------\\\\n\");\n",
    "\t\tprintf(\"This kernel was launched over a grid consisting of %d blocks,\\\\n\", gridDim.x);\n",
    "\t\tprintf(\"where each block has %d threads.\\\\n\", blockDim.x);\n",
    "\t}\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "hello_ker = ker.get_function(\"hello_world_ker\")\n",
    "hello_ker(block=(5, 1, 1), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does our kernel work correctly? : True\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/Chapter04/simple_scalar_multiply_kernel.py\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "ker = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void scalar_multiply_kernel(float *outvec, float scalar, float *vec)\n",
    "{\n",
    "     int i = threadIdx.x;\n",
    "     outvec[i] = scalar*vec[i];\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "scalar_multiply_gpu = ker.get_function(\"scalar_multiply_kernel\")\n",
    "\n",
    "testvec = np.random.randn(512).astype(np.float32)\n",
    "testvec_gpu = gpuarray.to_gpu(testvec)\n",
    "outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
    "\n",
    "scalar_multiply_gpu(outvec_gpu, np.float32(2), testvec_gpu, block=(512, 1, 1), grid=(1, 1, 1))\n",
    "\n",
    "print(f\"Does our kernel work correctly? : {np.allclose(outvec_gpu.get() , 2*testvec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0, 0): Block (1, 0, 0), Thread (0, 0, 0)\n",
      "(5, 0, 0): Block (1, 0, 0), Thread (1, 0, 0)\n",
      "(6, 0, 0): Block (1, 0, 0), Thread (2, 0, 0)\n",
      "(7, 0, 0): Block (1, 0, 0), Thread (3, 0, 0)\n",
      "(4, 1, 0): Block (1, 0, 0), Thread (0, 1, 0)\n",
      "(5, 1, 0): Block (1, 0, 0), Thread (1, 1, 0)\n",
      "(6, 1, 0): Block (1, 0, 0), Thread (2, 1, 0)\n",
      "(7, 1, 0): Block (1, 0, 0), Thread (3, 1, 0)\n",
      "(4, 0, 1): Block (1, 0, 0), Thread (0, 0, 1)\n",
      "(5, 0, 1): Block (1, 0, 0), Thread (1, 0, 1)\n",
      "(6, 0, 1): Block (1, 0, 0), Thread (2, 0, 1)\n",
      "(7, 0, 1): Block (1, 0, 0), Thread (3, 0, 1)\n",
      "(4, 1, 1): Block (1, 0, 0), Thread (0, 1, 1)\n",
      "(5, 1, 1): Block (1, 0, 0), Thread (1, 1, 1)\n",
      "(6, 1, 1): Block (1, 0, 0), Thread (2, 1, 1)\n",
      "(7, 1, 1): Block (1, 0, 0), Thread (3, 1, 1)\n",
      "(0, 0, 0): Block (0, 0, 0), Thread (0, 0, 0)\n",
      "(1, 0, 0): Block (0, 0, 0), Thread (1, 0, 0)\n",
      "(2, 0, 0): Block (0, 0, 0), Thread (2, 0, 0)\n",
      "(3, 0, 0): Block (0, 0, 0), Thread (3, 0, 0)\n",
      "(0, 1, 0): Block (0, 0, 0), Thread (0, 1, 0)\n",
      "(1, 1, 0): Block (0, 0, 0), Thread (1, 1, 0)\n",
      "(2, 1, 0): Block (0, 0, 0), Thread (2, 1, 0)\n",
      "(3, 1, 0): Block (0, 0, 0), Thread (3, 1, 0)\n",
      "(0, 0, 1): Block (0, 0, 0), Thread (0, 0, 1)\n",
      "(1, 0, 1): Block (0, 0, 0), Thread (1, 0, 1)\n",
      "(2, 0, 1): Block (0, 0, 0), Thread (2, 0, 1)\n",
      "(3, 0, 1): Block (0, 0, 0), Thread (3, 0, 1)\n",
      "(0, 1, 1): Block (0, 0, 0), Thread (0, 1, 1)\n",
      "(1, 1, 1): Block (0, 0, 0), Thread (1, 1, 1)\n",
      "(2, 1, 1): Block (0, 0, 0), Thread (2, 1, 1)\n",
      "(3, 1, 1): Block (0, 0, 0), Thread (3, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\n",
    "    \"\"\"\n",
    "__global__ void test_kernel()\n",
    "{\n",
    "    #define MY_X (threadIdx.x + blockIdx.x * blockDim.x)\n",
    "    #define MY_Y (threadIdx.y + blockIdx.y * blockDim.y)\n",
    "    #define MY_Z (threadIdx.z + blockIdx.z * blockDim.z)\n",
    "\n",
    "    //printf(\"gridDim %d %d %d, warp %d\\\\n\", gridDim.x, gridDim.y, gridDim.z, warpSize);\n",
    "    //printf(\"Hello world from thread %d, in block %d!\\\\n\", threadIdx.x, blockIdx.x);\n",
    "    printf(\"(%d, %d, %d): Block (%d, %d, %d), Thread (%d, %d, %d)\\\\n\", MY_X, MY_Y, MY_Z, blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, threadIdx.y, threadIdx.z);\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "test_kernel = mod.get_function(\"test_kernel\")\n",
    "test_kernel(block=(4, 2, 2), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter\n",
    "At this point I could make a PyCUDA cell magic :)\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/config/custommagics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_magic, register_cell_magic, register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def cmagic(line, cell):\n",
    "    \"my cell magic\"\n",
    "    return line, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('foo', '\\nthis is a test\\n\\nfoo bar\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%cmagic foo\n",
    "\n",
    "this is a test\n",
    "\n",
    "foo bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def cuda(line, cell):\n",
    "    \"Runs NVIDIA CUDA C code via PyCUDA\"\n",
    "    notebook_path = os.path.abspath(\"\")\n",
    "    mod = SourceModule(cell)\n",
    "    run_me = mod.get_function(\"run_me\")\n",
    "    run_me(block=(4, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "hello world\n",
      "hello world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "__global__ void run_me()\n",
    "{\n",
    "    printf(\"hello world\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cuda test_kernel arg1 arg2 block=(1,2,3) grid=(4,5,6)\n",
    "# requires parsing ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no syntax highlighting in magic cells :(\n",
    "\n",
    "maybe a custom CUDA Kernel?\n",
    "\n",
    "https://jupyter-client.readthedocs.io/en/latest/kernels.html\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/install/kernel_install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CudaSourceFile\n",
    "Instead of going the Jupyter route, I'll just use external files for all the IDE\n",
    "goodness. Also enables reusability like headerfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0, 0): Block (1, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(5, 0, 0): Block (1, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(6, 0, 0): Block (1, 0, 0), Thread (2, 0, 0) -- 42\n",
      "(7, 0, 0): Block (1, 0, 0), Thread (3, 0, 0) -- 42\n",
      "(4, 1, 0): Block (1, 0, 0), Thread (0, 1, 0) -- 42\n",
      "(5, 1, 0): Block (1, 0, 0), Thread (1, 1, 0) -- 42\n",
      "(6, 1, 0): Block (1, 0, 0), Thread (2, 1, 0) -- 42\n",
      "(7, 1, 0): Block (1, 0, 0), Thread (3, 1, 0) -- 42\n",
      "(4, 0, 1): Block (1, 0, 0), Thread (0, 0, 1) -- 42\n",
      "(5, 0, 1): Block (1, 0, 0), Thread (1, 0, 1) -- 42\n",
      "(6, 0, 1): Block (1, 0, 0), Thread (2, 0, 1) -- 42\n",
      "(7, 0, 1): Block (1, 0, 0), Thread (3, 0, 1) -- 42\n",
      "(4, 1, 1): Block (1, 0, 0), Thread (0, 1, 1) -- 42\n",
      "(5, 1, 1): Block (1, 0, 0), Thread (1, 1, 1) -- 42\n",
      "(6, 1, 1): Block (1, 0, 0), Thread (2, 1, 1) -- 42\n",
      "(7, 1, 1): Block (1, 0, 0), Thread (3, 1, 1) -- 42\n",
      "(0, 0, 0): Block (0, 0, 0), Thread (0, 0, 0) -- 42\n",
      "(1, 0, 0): Block (0, 0, 0), Thread (1, 0, 0) -- 42\n",
      "(2, 0, 0): Block (0, 0, 0), Thread (2, 0, 0) -- 42\n",
      "(3, 0, 0): Block (0, 0, 0), Thread (3, 0, 0) -- 42\n",
      "(0, 1, 0): Block (0, 0, 0), Thread (0, 1, 0) -- 42\n",
      "(1, 1, 0): Block (0, 0, 0), Thread (1, 1, 0) -- 42\n",
      "(2, 1, 0): Block (0, 0, 0), Thread (2, 1, 0) -- 42\n",
      "(3, 1, 0): Block (0, 0, 0), Thread (3, 1, 0) -- 42\n",
      "(0, 0, 1): Block (0, 0, 0), Thread (0, 0, 1) -- 42\n",
      "(1, 0, 1): Block (0, 0, 0), Thread (1, 0, 1) -- 42\n",
      "(2, 0, 1): Block (0, 0, 0), Thread (2, 0, 1) -- 42\n",
      "(3, 0, 1): Block (0, 0, 0), Thread (3, 0, 1) -- 42\n",
      "(0, 1, 1): Block (0, 0, 0), Thread (0, 1, 1) -- 42\n",
      "(1, 1, 1): Block (0, 0, 0), Thread (1, 1, 1) -- 42\n",
      "(2, 1, 1): Block (0, 0, 0), Thread (2, 1, 1) -- 42\n",
      "(3, 1, 1): Block (0, 0, 0), Thread (3, 1, 1) -- 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "\n",
    "\n",
    "class CudaSourceFile:\n",
    "    def __init__(\n",
    "        self, filename: str, kernels: list[str] = list(), include_dirs: list[str] = list()\n",
    "    ) -> None:\n",
    "        with open(filename) as f:\n",
    "            file_str = f.read()\n",
    "            self.mod = SourceModule(file_str, include_dirs=include_dirs)\n",
    "\n",
    "        for k in kernels:\n",
    "            setattr(self, k, self.mod.get_function(k))\n",
    "\n",
    "\n",
    "cf = CudaSourceFile(\"test_kernel.cu\", kernels=[\"test_kernel\"], include_dirs=[notebook_path])\n",
    "cf.test_kernel(block=(4, 2, 2), grid=(2, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Transfers\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations\n",
    "> Memory optimizations are the most important area for performance.\n",
    "\n",
    "> it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.\n",
    "\n",
    "> Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.\n",
    "\n",
    "> higher bandwidth between the host and the device is achieved when using page-locked (or pinned) memory\n",
    "\n",
    "---\n",
    "\n",
    "Some useful hints:\n",
    "- https://medium.com/@rupertt/accelerate-computation-with-pycuda-2c12a6555cc6\n",
    "  - cuda.memcpy_htod\n",
    "  - cuda.memcpy_dtoh\n",
    "- https://wlandau.github.io/gpu/lectures/pycuda/pycuda.pdf\n",
    "  - In, Out, InOut\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [   0  256  512  768 1024 1280 1536 1792    1  257  513  769 1025 1281\n",
      " 1537 1793    2  258  514  770 1026 1282 1538 1794    3  259  515  771\n",
      " 1027 1283 1539 1795]\n",
      "npout [   0  256  512  768 1024 1280 1536 1792    1  257  513  769 1025 1281\n",
      " 1537 1793    2  258  514  770 1026 1282 1538 1794    3  259  515  771\n",
      " 1027 1283 1539 1795]\n",
      "npout[10] 513\n",
      "npout pair (2, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0): out[0] = 0\n",
      "(1, 0): out[1] = 256\n",
      "(2, 0): out[2] = 512\n",
      "(3, 0): out[3] = 768\n",
      "(4, 0): out[4] = 1024\n",
      "(5, 0): out[5] = 1280\n",
      "(6, 0): out[6] = 1536\n",
      "(7, 0): out[7] = 1792\n",
      "(0, 1): out[8] = 1\n",
      "(1, 1): out[9] = 257\n",
      "(2, 1): out[10] = 513\n",
      "(3, 1): out[11] = 769\n",
      "(4, 1): out[12] = 1025\n",
      "(5, 1): out[13] = 1281\n",
      "(6, 1): out[14] = 1537\n",
      "(7, 1): out[15] = 1793\n",
      "(0, 2): out[16] = 2\n",
      "(1, 2): out[17] = 258\n",
      "(2, 2): out[18] = 514\n",
      "(3, 2): out[19] = 770\n",
      "(4, 2): out[20] = 1026\n",
      "(5, 2): out[21] = 1282\n",
      "(6, 2): out[22] = 1538\n",
      "(7, 2): out[23] = 1794\n",
      "(0, 3): out[24] = 3\n",
      "(1, 3): out[25] = 259\n",
      "(2, 3): out[26] = 515\n",
      "(3, 3): out[27] = 771\n",
      "(4, 3): out[28] = 1027\n",
      "(5, 3): out[29] = 1283\n",
      "(6, 3): out[30] = 1539\n",
      "(7, 3): out[31] = 1795\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_pair(n):\n",
    "    return (n >> 8, n & 0xFF)\n",
    "\n",
    "\n",
    "cf = CudaSourceFile(\"test_indicies.cu\", kernels=[\"test_indicies\"], include_dirs=[notebook_path])\n",
    "\n",
    "cols = 8\n",
    "rows = 4\n",
    "out = cuda.managed_empty(shape=rows * cols, dtype=np.int32, mem_flags=cuda.mem_attach_flags.GLOBAL)\n",
    "cf.test_indicies(np.int32(cols), out, block=(8, 4, 1), grid=(1, 1, 1))\n",
    "print(\"out:\", out)\n",
    "npout = np.array(out)\n",
    "print(\"npout\", npout)\n",
    "print(\"npout[10]\", npout[10])\n",
    "print(\"npout pair\", to_pair(npout[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## managed_empty\n",
    "\n",
    "https://documen.tician.de/pycuda/driver.html#managed-memory\n",
    "\n",
    "> CUDA 6.0 adds support for a “Unified Memory” model, which creates a managed virtual memory space that is visible to both CPUs and GPUs. The OS will migrate the physical pages associated with managed memory between the CPU and GPU as needed. This allows a numpy array on the host to be passed to kernels without first creating a DeviceAllocation and manually copying the host data to and from the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "\n",
    "out = cuda.managed_empty(shape=(3, 2), dtype=np.int32, mem_flags=cuda.mem_attach_flags.GLOBAL)\n",
    "cf.test_pairs(out, block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"out:\", out)\n",
    "npout = np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## malloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npout [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n",
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "rows = 3\n",
    "cols = 2\n",
    "sizeof_int = 4\n",
    "out = cuda.mem_alloc(rows * cols * sizeof_int)\n",
    "cf.test_pairs(out, block=(3, 1, 1), grid=(1, 1, 1))\n",
    "npout = np.empty((3, 2), dtype=np.int32)\n",
    "cuda.memcpy_dtoh(npout, out)\n",
    "print(\"npout\", npout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pycuda.Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: [[ 10 100]\n",
      " [ 11 100]\n",
      " [ 12 100]]\n",
      "0: (10, 100)\n",
      "1: (11, 100)\n",
      "2: (12, 100)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "res = np.empty((3, 2), dtype=np.int32)\n",
    "cf.test_pairs(cuda.Out(res), block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"res:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from_device / device allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"test_pairs.cu\", kernels=[\"test_pairs\"], include_dirs=[notebook_path])\n",
    "\n",
    "res = np.empty((3, 2), dtype=np.int32)\n",
    "cf.test_pairs(cuda.Out(res), block=(3, 1, 1), grid=(1, 1, 1))\n",
    "print(\"res:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pycuda.In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buf [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "(0, 0): val 0\n",
      "(1, 0): val 1\n",
      "(2, 0): val 2\n",
      "(0, 1): val 3\n",
      "(1, 1): val 4\n",
      "(2, 1): val 5\n",
      "(0, 2): val 6\n",
      "(1, 2): val 7\n",
      "(2, 2): val 8\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"dump_buf.cu\", kernels=[\"dump_buf\"], include_dirs=[notebook_path])\n",
    "\n",
    "input_buf = np.arange(12).reshape((3, 4)).astype(np.int8)\n",
    "width = 3\n",
    "height = 2\n",
    "cf.dump_buf(np.int32(width), np.int32(height), cuda.In(input_buf), block=(3, 3, 1))\n",
    "cuda.context.synchronize()\n",
    "print(\"input_buf\", input_buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- [ ] from_device\n",
    "  - https://documen.tician.de/pycuda/driver.html#pycuda.driver.from_device\n",
    "- [ ] mem_alloc_pitch\n",
    "  - https://documen.tician.de/pycuda/driver.html#pycuda.driver.mem_alloc_pitch\n",
    "- [ ] page-locked host memory\n",
    "  - https://documen.tician.de/pycuda/driver.html#pagelocked-host-memory\n",
    "  - https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory\n",
    "- [ ] memory pools\n",
    "  - https://documen.tician.de/pycuda/util.html#device-based-memory-pool\n",
    "- [ ] structs\n",
    "  - https://github.com/minrk/PyCUDA/blob/master/doc/source/tutorial.rst#structures\n",
    "- [ ] Shared memory\n",
    "  - https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 16) (966704592.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"grid\\ngrid)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 16)\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "cf = CudaSourceFile(\"fe_single.cu\", kernels=[\"fe_single\"], include_dirs=[notebook_path])\n",
    "\n",
    "grid = np.array(\n",
    "    [\n",
    "        [3, 3, 0, 5],\n",
    "        [0, 1, 0, 5],\n",
    "        [0, 0, 4, 2],\n",
    "    ],\n",
    "    dtype=np.int16,\n",
    ")\n",
    "print(f\"grid\\n{grid}\")\n",
    "\n",
    "res = np.zeros_like(grid)\n",
    "\n",
    "width = grid.shape[0]\n",
    "height = grid.shape[1]\n",
    "np_width = np.int32(width)\n",
    "np_height = np.int32(height)\n",
    "\n",
    "cf.fe_single(cuda.Out(res), np_width, np_height, cuda.In(grid), block=(width, height, 1))\n",
    "print(f\"res:\\n{res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ROC)",
   "language": "python",
   "name": "python-poetry-roc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
